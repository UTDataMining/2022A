{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UTDataMining/2022A/blob/main/ex2/ex2_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY_7tUgrzasL"
      },
      "source": [
        "# EX2 Data Handling\n",
        "\n",
        "Points\n",
        "- Q1 4P\n",
        "- Q2-1 1P\n",
        "- Q2-2 2P\n",
        "- Q2-3 1P\n",
        "- Q2-4 2P\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaePYiuK0Dmd"
      },
      "source": [
        "## Q1\n",
        "### Median value\n",
        "The median is a measure of descriptive statistics for observing the central tendency of data. Compared to the mean, the median is less affected by outliers in data.\n",
        "\n",
        "- if the number of data, $n$, is odd, the median value is $(n+1)/2$ th element sorted by the values of data\n",
        "- if the number of data, $n$, is even, the median value is the average of $n/2$ th and $n/2+1$ th elements sorted by the values of data\n",
        "\n",
        "### Variance\n",
        "The variance is a measure of descriptive statistics for observing the variation in data. It represents how much the individual sample deviates from the mean of the data. \n",
        "\n",
        "As shown below, the variance is the square sum of the differences between the individual sample $x_{i}$ and the mean of the data $\\overline{x}$ divided by the number of data $n$:\n",
        "\n",
        "$\\frac{1}{n}\\Sigma^{n}_{i=1}(x_{i}-\\overline{x})^{2}$.\n",
        "\n",
        "Complete the `avg_med_var()` function, which takes a list of any length with elements of integers between 0 and 100 as the argument `input_list` and returns a list with the following elements.\n",
        "- mean value of all elements of the input list\n",
        "- median value of all elements of the input list\n",
        "- variance value of all elements of the input list\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBXUEkWjzWbZ",
        "exercise_id": "q1",
        "inlinetests": {
          "InlineTest_1": "\nassert 'avg_med_var' in globals(), f\"Have you defined the function 'avg_med_var'?\"\nassert str(avg_med_var.__class__) == \"<class 'function'>\", f\"Have you defined a function 'avg_med_var'? Found a {avg_med_var.__class__} instead\"\ntry:\n  import random\n  import numpy as np\n  #lst = [random.randint(0, 20) for i in range(10)]\n  lst = [1,4,3,2,5,6,8,7,9,10]\n  ans = [np.mean(lst), np.median(lst), np.var(lst)]\n  assert avg_med_var(lst) == ans, f\"Your function 'avg_med_var' returns {avg_med_var(lst)}, while the expected answer is {ans}\"\n  lst = [1,4,3,2,5,6,8,7,9,10,11]\n  ans = [np.mean(lst), np.median(lst), np.var(lst)]\n  assert avg_med_var(lst) == ans, f\"Your function 'avg_med_var' returns {avg_med_var(lst)}, while the expected answer is {ans}\"\nexcept AssertionError as e:\n    raise e\nexcept Exception as e:\n    assert False, f\"Your function raises an exception: {e}.\""
        }
      },
      "source": [
        "def avg_med_var(input_list):\n",
        "  mean_value=0 \n",
        "  med_value=0 \n",
        "  var_value=0 \n",
        "  results=[] \n",
        "        \n",
        "  input_list.sort() \n",
        "  mid_idx = len(input_list)//2 \n",
        "\n",
        "  if ...\n",
        "    med_value= ... # median\n",
        "  else:\n",
        "    med_value = ... # median\n",
        "    \n",
        "  mean_value = ... #mean\n",
        "  mse_value = 0 # initializing the square sum\n",
        "    \n",
        "  for ...\n",
        "    mse_value = # adding the square sum of the difference between the individual sample and the mean\n",
        "  var_value = ... # variance\n",
        "    \n",
        "  results.append(...) \n",
        "  results.append(...) \n",
        "  results.append(...)\n",
        "\n",
        "  return results   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Tw1UuQ1izK"
      },
      "source": [
        "Once the `avg_med_var` function is complete, run the following cell to see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuuDCRFf1jRo"
      },
      "source": [
        "print(avg_med_var([1,4,3,2,5,6,8,7,9,10]))\n",
        "print(avg_med_var([1,4,3,2,5,6,8,7,9,10,11]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlYmaunz14pT"
      },
      "source": [
        "Make sure that your function returns `[5.5, 5.5, 8.25]` and `[6.0, 6.0, 10.0]`, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMTTIHWW4Lm9"
      },
      "source": [
        "# Q2\n",
        "Create a dataframe `score` by reading the following \"user_score.csv\" file.\n",
        "```Python\n",
        "## user_score.csvファイル\n",
        "user, kokugo, shakai, sugaku, rika\n",
        "1, 30, 43, 51, 63\n",
        "2, 39, 21, 49, 56\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbQTRbRm4Sm2"
      },
      "source": [
        "# Colaboratoryでは以下を実行して必要なファイルをダウンロード\n",
        "!wget https://raw.githubusercontent.com/UTDataMining/2022A/master/ex2/user_score.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urTOeAV34aW5"
      },
      "source": [
        "import pandas as pd\n",
        "score = pd.read_csv('user_score.csv')\n",
        "score.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoexDTJH489m"
      },
      "source": [
        "Show the descriptive statistics using a `describe()` method.\n",
        "\n",
        "[pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfDVrnL749hc"
      },
      "source": [
        "score.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkqdvxR5MkP"
      },
      "source": [
        "### Q2-1\n",
        "Complete the `above_mean()` function that takes the dataframe `score` with argument `df` and returns the number of `users` whom their scores are above the mean for a given `subject`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4fXMocB5Lpw",
        "exercise_id": "q2_1",
        "inlinetests": {
          "InlineTest_2_1": "\nassert 'above_mean' in globals(), f\"Have you defined the function 'above_mean'?\"\nassert str(above_mean.__class__) == \"<class 'function'>\", f\"Have you defined a function 'avg_med_var'? Found a {above_mean.__class__} instead\"\ntry:\n  assert above_mean(score, 'kokugo') == len(score[(score['kokugo']>=score['kokugo'].mean())]), f\"Your function 'above_mean' returns {above_mean(score, 'kokugo')}, while the expected answer is {len(score[(score['kokugo']>=score['kokugo'].mean())])}\"\n  assert above_mean(score, 'sugaku') == len(score[(score['sugaku']>=score['sugaku'].mean())]), f\"Your function 'above_mean' returns {above_mean(score, 'sugaku')}, while the expected answer is {len(score[(score['sugaku']>=score['sugaku'].mean())])}\"\nexcept AssertionError as e:\n    raise e\nexcept Exception as e:\n    assert False, f\"Your function raises an exception: {e}.\""
        }
      },
      "source": [
        "def above_mean(df, subject):\n",
        "  ...\n",
        "  return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U70gq8sp6YtU"
      },
      "source": [
        "Once the `above_mean` function is complete, run the following cell to see how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNtw_xWx6ijA"
      },
      "source": [
        "above_mean(score, 'kokugo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbQO4GBj6phS"
      },
      "source": [
        "Make sure that your function returns `84` given a subject 'kokugo'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_l5CPKW9Fvs"
      },
      "source": [
        "### Q2-2\n",
        "First create the `score_sum()` function that takes the dataframe `score` with argument `df` and returns a dataframe with a new 'sum' column representing the sum of the scores of all subjects for each user.\n",
        "\n",
        "```Python\n",
        "データフレーム[[列1,列2,...,]].sum(axis=1)\n",
        "```\n",
        "[pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtgbOc_W_6W8"
      },
      "source": [
        "def score_sum(df):\n",
        "  df['sum'] = df[['kokugo','shakai','sugaku','rika']].sum(axis=1)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qI_GAMx_-ox"
      },
      "source": [
        "Run the following cell and make sure that a new column, 'sum', is added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQCIPx7pAE_A"
      },
      "source": [
        "score = score_sum(score)\n",
        "score.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erqWC7mQAd6x"
      },
      "source": [
        "Complete the `score_top3()` function that takes the dataframe `score` with argument `df` and return a list with the top three 'users' of total scores, 'sum', as elements in order of first, second and third place.\n",
        "\n",
        "Note that you can assume that there is no identical ranking in 'sum'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgYvXeI-9J5J",
        "exercise_id": "q2_2",
        "inlinetests": {
          "InlineTest_2_2": "\nassert 'score_top3' in globals(), f\"Have you defined the function 'score_top3'?\"\nassert str(score_top3.__class__) == \"<class 'function'>\", f\"Have you defined a function 'score_top3'? Found a {score_top3.__class__} instead\"\ntry:\n  assert score_top3(score) == [39,4,50], f\"Your function 'score_top3' returns {score_top3(score)}, while the expected answer is {[39,4,50]}\"\nexcept AssertionError as e:\n    raise e\nexcept Exception as e:\n    assert False, f\"Your function raises an exception: {e}.\""
        }
      },
      "source": [
        "def score_top3(df):\n",
        "  ...\n",
        "  return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFKDPh0jA7JT"
      },
      "source": [
        "Once the `score_top3` function is complete, run the following cell to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAsHBYweA9lD"
      },
      "source": [
        "score_top3(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuAJPZEvA9x3"
      },
      "source": [
        "Make sure that your function returns `[39, 4, 50]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d64XIy9iBswp"
      },
      "source": [
        "### Q2-3\n",
        "Complete the `score_hist()` function that takes the data frame `score` with argument `df` and returns a list with the frequencies of each class as elements, with the following class widths: \n",
        "- 0 or more, 50 or less\n",
        "- 51 or more, 100 or less\n",
        "- 101 or more, 150 or less\n",
        "- 151 or more, 200 or less\n",
        "- 201 or more, 250 or less\n",
        "- 251 or  more, 300 or less\n",
        "- 301 or more, 350 or less\n",
        "- 351 or more, 400 or less"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3S2vE4bBw12",
        "exercise_id": "q2_3",
        "inlinetests": {
          "InlineTest_2_3": "\nassert 'score_hist' in globals(), f\"Have you defined the function 'score_hist'?\"\nassert str(score_hist.__class__) == \"<class 'function'>\", f\"Have you defined a function 'score_hist'? Found a {score_hist.__class__} instead\"\ntry:\n  ans = [4, 20, 39, 35, 29, 24, 13, 2]\n  assert score_hist(score) == ans, f\"Your function 'score_hist' returns {score_hist(score)}, while the expected answer is {ans}\"\nexcept AssertionError as e:\n    raise e\nexcept Exception as e:\n    assert False, f\"Your function raises an exception: {e}.\""
        }
      },
      "source": [
        "def score_hist(df):\n",
        "  ...\n",
        "  return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4wU0iH8Ee_m"
      },
      "source": [
        "Once the `score_hist` function is complete, run the following cell to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WvkWUA_EgrS"
      },
      "source": [
        "score_hist(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3-mLOn5EhQ8"
      },
      "source": [
        "Make sure that your function returns `[4, 20, 39, 35, 29, 24, 13, 2]`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k18pw5IVEx9_"
      },
      "source": [
        "### Q2-4\n",
        "Create a data frame `user_class` by reading the following \"user_class.csv\" file.\n",
        "```Python\n",
        "## user_class.csvファイル\n",
        "'user', 'class'\n",
        "1, 'C'\n",
        "2, 'C'\n",
        "3, 'B'\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlq5fwWeE2nh"
      },
      "source": [
        "# Colaboratoryでは以下を実行して必要なファイルをダウンロード\n",
        "!wget https://raw.githubusercontent.com/UTDataMining/2022A/master/ex2/user_class.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yVd8d1NFwGI"
      },
      "source": [
        "user_class = pd.read_csv('user_class.csv')\n",
        "user_class.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqJntUKIF-VN"
      },
      "source": [
        "Complete the `score_by_class()` function that takes two dataframes, `user_class` and `score`, with arguments `df1` and `df2` respectively, creates a new dataframe by combining the two dataframes with the 'user' as key, and return a series object with the 'class' as its index and the highest value 'sum' per 'class' as its column using a `groupby` method of the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHiE6F9sF9Ty",
        "exercise_id": "q2_4",
        "inlinetests": {
          "InlineTest_2_4": "\nassert 'score_by_class' in globals(), f\"Have you defined the function 'score_by_class'?\"\nassert str(score_by_class.__class__) == \"<class 'function'>\", f\"Have you defined a function 'score_by_class'? Found a {score_by_class.__class__} instead\"\ntry:\n  ans = {'A': 361, 'B': 349, 'C': 359}\n  assert score_by_class(score, user_class).to_dict() == ans, f\"Your function 'score_by_class' returns {score_by_class(score, user_class).to_dict()}, while the expected answer is {ans}\"\nexcept AssertionError as e:\n    raise e\nexcept Exception as e:\n    assert False, f\"Your function raises an exception: {e}.\""
        }
      },
      "source": [
        "def score_by_class(df1, df2):\n",
        "  ...\n",
        "  return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3osQUPmHHvB"
      },
      "source": [
        "Once the `score_by_class` function is complete, run the following cell to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmoP7lH5HNOX"
      },
      "source": [
        "score_by_class(score, user_class).to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8_gks5HN18"
      },
      "source": [
        "It should return `{'A': 361, 'B': 349, 'C': 359}`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9paB_oYvKq3J"
      },
      "source": [
        "# Code Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8EGGkUnKsWL"
      },
      "source": [
        "# Run this cell first\n",
        "!pip install prog_edu_assistant_tools\n",
        "import re\n",
        "import sys\n",
        "import jinja2\n",
        "from IPython.core import display\n",
        "from google.colab import _message as google_message\n",
        "from prog_edu_assistant_tools.magics import report, autotest, CaptureOutput\n",
        "from prog_edu_assistant_tools.check import Check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBmME2BCK3pa"
      },
      "source": [
        "## Q1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8--IsILXK24k"
      },
      "source": [
        "# Run this cell to check your solution.\n",
        "# If you get an error 'Check not defined', make sure you have run all preceding\n",
        "# cells once (Runtime -> Run before)\n",
        "Check('q1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghtta35YLCCK"
      },
      "source": [
        "## Q2-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5weMT7X5LFMz"
      },
      "source": [
        "# Run this cell to check your solution.\n",
        "# If you get an error 'Check not defined', make sure you have run all preceding\n",
        "# cells once (Runtime -> Run before)\n",
        "Check('q2_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrZnK4mLLjz"
      },
      "source": [
        "## Q2-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuuzw3aoLM-C"
      },
      "source": [
        "# Run this cell to check your solution.\n",
        "# If you get an error 'Check not defined', make sure you have run all preceding\n",
        "# cells once (Runtime -> Run before)\n",
        "Check('q2_2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5SR1hBLQ0y"
      },
      "source": [
        "## Q2-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydSwT6VjLSTb"
      },
      "source": [
        "# Run this cell to check your solution.\n",
        "# If you get an error 'Check not defined', make sure you have run all preceding\n",
        "# cells once (Runtime -> Run before)\n",
        "Check('q2_3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1sEH6YBLUIy"
      },
      "source": [
        "## Q2-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaK2X8uYLWCT"
      },
      "source": [
        "# Run this cell to check your solution.\n",
        "# If you get an error 'Check not defined', make sure you have run all preceding\n",
        "# cells once (Runtime -> Run before)\n",
        "Check('q2_4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}